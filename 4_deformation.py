#ç›´æ¥è®¡ç®—å‹å˜é‡ä¸ç”¨è¿­ä»£
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import os
from PIL import Image
from scipy.optimize import linear_sum_assignment
from tqdm import tqdm
# å‚æ•°
num_point = 8000
template_path = r"/home/mozhengao/code/particle/data2_simulate/10345/j97_binary.png"
particles_path = r"/home/mozhengao/code/particle/data2_simulate/pic_part/Bottom_Right"
output_file_path = r"/home/mozhengao/code/particle/data2_simulate/pic_part/TBottom_Right_sort.txt"  # ä¿å­˜çš„txtæ–‡ä»¶è·¯å¾„
outputfile_region = r"/home/mozhengao/code/particle/data2_simulate/pic_part/Bottom_Right_region.txt" #ä¿å­˜å½¢å˜æƒ…å†µçš„
output_all = r"/home/mozhengao/code/particle/data2_simulate/pic_part/Bottom_Right.png"
device = torch.device("cuda:1" if torch.cuda.is_available() else "cpu")

deformation_accumulator = np.zeros(num_point, dtype=np.float32)
count_accumulator = np.zeros(num_point, dtype=np.float32)

image = Image.open(template_path)
img_width, img_height = image.size

def update_template_deformation(file_name, B_points, template_points, region_labels, deformation_accumulator, count_accumulator):
    """
    è®¡ç®—å½“å‰å›¾ç‰‡å¯¹åº”çš„æ¨¡æ¿é¢—ç²’å½¢å˜å€¼ï¼Œå¹¶æ›´æ–°ç´¯ç§¯å½¢å˜å€¼ï¼ˆè®¡ç®—å¹³å‡å½¢å˜ï¼‰

    å‚æ•°:
    - file_name: str, å½“å‰å¤„ç†çš„å›¾ç‰‡æ–‡ä»¶å
    - B_points: ndarray, å½“å‰å›¾ç‰‡çš„ç‚¹äº‘æ•°æ® (N, 3)
    - template_points: ndarray, æ¨¡æ¿ç‚¹äº‘æ•°æ® (N, 3)
    - region_labels: ndarray, æŒ‡ç¤ºæ¯ä¸ªç‚¹å±äºå“ªä¸ªåŒºåŸŸ (N,)
    - deformation_accumulator: ndarray, å­˜å‚¨æ‰€æœ‰å›¾ç‰‡ç´¯è®¡å½¢å˜é‡ (N,)
    - count_accumulator: ndarray, å­˜å‚¨æ¯ä¸ªç‚¹çš„ç´¯è®¡è®¡ç®—æ¬¡æ•° (N,)

    è¿”å›:
    - updated_deformation: ndarray, æ›´æ–°åçš„æ¨¡æ¿å¹³å‡å½¢å˜é‡ (N,)
    """
    # ç¡®ä¿ B_points å’Œ template_points æ˜¯ NumPy æ•°ç»„
    if isinstance(B_points, torch.Tensor):
        B_points = B_points.detach().cpu().numpy()
    if isinstance(template_points, torch.Tensor):
        template_points = template_points.detach().cpu().numpy()

    B_points = B_points.squeeze(0)  # å˜æˆ (8000, 2)
    template_points = template_points.squeeze(0)  # å˜æˆ (8000, 2)

    deformation_current = np.linalg.norm(B_points - template_points, axis=1)  # ç¡®ä¿è¾“å‡ºå½¢çŠ¶æ˜¯ (N,)

    # ç´¯åŠ å½¢å˜é‡
    deformation_accumulator += deformation_current
    count_accumulator += 1  # è®°å½•æ¯ä¸ªç‚¹è¢«å¤„ç†çš„æ¬¡æ•°

    # è®¡ç®—å½“å‰çš„å¹³å‡å½¢å˜å€¼
    updated_deformation = deformation_accumulator / count_accumulator

    return updated_deformation


def plot_deformation_with_template(points, deformations, output_all):
    """
    ç»˜åˆ¶æ¨¡æ¿ç‚¹äº‘çš„å½¢å˜é‡å¯è§†åŒ–

    å‚æ•°:
    - points: Tensor/ndarray, æ¨¡æ¿ç‚¹äº‘æ•°æ® (N, 3)
    - deformations: Tensor/ndarray, æ¯ä¸ªç‚¹çš„å½¢å˜é‡ (N,)
    - output_path: str, ä¿å­˜å¯è§†åŒ–å›¾åƒçš„è·¯å¾„
    """
    plt.figure(figsize=(8, 8))

    # ğŸš€ ç¡®ä¿ points å’Œ deformations åœ¨ CPU å¹¶è½¬ä¸º NumPy æ•°ç»„
    if isinstance(points, torch.Tensor):
        points = points.detach().cpu().numpy()

    if isinstance(deformations, torch.Tensor):
        deformations = deformations.detach().cpu().numpy()

    # ç¡®ä¿ points å½¢çŠ¶æ˜¯ (N, 2) è€Œä¸æ˜¯ (1, N, 2)
    if points.ndim == 3 and points.shape[0] == 1:
        points = points.squeeze(0)

    # ç¡®ä¿ deformations å½¢çŠ¶æ˜¯ (N, 1) æˆ– (N,)
    if deformations.ndim == 2 and deformations.shape[1] == 1:
        deformations = deformations.squeeze(1)

    # ç¡®ä¿å½¢çŠ¶åŒ¹é…
    assert deformations.shape[0] == points.shape[0], f"Shape mismatch: {deformations.shape} vs {points.shape}"

    # å½’ä¸€åŒ–å½¢å˜é‡ï¼Œè°ƒæ•´ç‚¹å¤§å°å’Œé€æ˜åº¦
    point_sizes = (deformations - deformations.min()) / (deformations.max() - deformations.min()) * 20 + 0.1
    point_alphas = (deformations - deformations.min()) / (deformations.max() - deformations.min()) * 0.8 + 0.2

    # ç»˜åˆ¶ç‚¹äº‘
    scatter = plt.scatter(
        points[:, 1],  # äº¤æ¢ x å’Œ y
        -points[:, 0],  # è´Ÿå·ç¿»è½¬ y è½´
        c=deformations, 
        cmap="viridis", 
        s=point_sizes, 
        alpha=point_alphas, 
        edgecolors="face",
    )

    plt.axis("equal")
    plt.axis("off")

    # ä¿å­˜å›¾åƒ
    plt.savefig(output_all, bbox_inches="tight", pad_inches=0, dpi=300)
    plt.close()

# åŒºåŸŸåˆ’åˆ†å‡½æ•°
def divide_points_into_quadrants(points, img_width, img_height):
    region_labels = []
    for point in points:
        x, y = point
        if x < img_width / 2 and y < img_height / 2:
            region_labels.append(0)  # å·¦ä¸Š
        elif x < img_width / 2 and y >= img_height / 2:
            region_labels.append(1)  # å³ä¸Š
        elif x >= img_width / 2 and y < img_height / 2:
            region_labels.append(2)  # å·¦ä¸‹
        else:
            region_labels.append(3)  # å³ä¸‹
    return np.array(region_labels)

# åŒºåŸŸå½¢å˜è®¡ç®—å‡½æ•°
def calculate_region_deformation(file_name, current_points, original_points, region_labels, outputfile_region, scale_max=100, b=3.5):
    """
    è®¡ç®—å››ä¸ªåŒºåŸŸçš„å¹³å‡å½¢å˜å€¼ï¼Œæ”¾å¤§è¾ƒå¤§åŒºåŸŸçš„æ•°å€¼ï¼Œå¹¶è®¡ç®—å æ¯”ï¼Œç„¶åå†™å…¥æ–‡ä»¶ï¼ˆè¦†ç›–ç›¸åŒæ–‡ä»¶åçš„æ•°æ®ï¼‰ã€‚

    å‚æ•°:
    - file_name: str, å½“å‰å¤„ç†çš„æ–‡ä»¶å
    - current_points: ndarray, å˜å½¢åçš„ç‚¹äº‘æ•°æ® (N, 3)
    - original_points: ndarray, åŸå§‹ç‚¹äº‘æ•°æ® (N, 3)
    - region_labels: ndarray, æŒ‡ç¤ºæ¯ä¸ªç‚¹å±äºå“ªä¸ªåŒºåŸŸ (N,)
    - scale_max: float, å½’ä¸€åŒ–åçš„æœ€å¤§å€¼ï¼ˆé»˜è®¤ 100ï¼‰
    - b: float, æŒ‡æ•°æ”¾å¤§ç³»æ•°ï¼ˆé»˜è®¤ 2ï¼‰

    è¿”å›:
    - deformation_percentages: list, å½’ä¸€åŒ–å¹¶æ”¾å¤§çš„åŒºåŸŸå½¢å˜é‡å æ¯”
    """

    # è¯»å–å·²æœ‰æ•°æ®ï¼Œå­˜å…¥å­—å…¸ {file_name: deformation_percentages}
    data_dict = {}
    if os.path.exists(outputfile_region):
        with open(outputfile_region, "r") as f:
            for line in f:
                parts = line.strip().split(",")
                if len(parts) == 5:  # ç¡®ä¿æ ¼å¼æ­£ç¡®
                    data_dict[parts[0]] = list(map(float, parts[1:]))

    # è®¡ç®—å½¢å˜é‡
    region_deformations = []
    for region in range(4):
        mask = (region_labels == region)
        if np.sum(mask) == 0:
            region_deformations.append(0)
            continue
        current_region_points = current_points[mask]
        original_region_points = original_points[mask]
        deformation = np.mean(np.linalg.norm(current_region_points - original_region_points, axis=1))
        region_deformations.append(deformation)

    # è½¬æ¢ä¸º NumPy æ•°ç»„
    region_deformations = np.array(region_deformations)

    # å½’ä¸€åŒ–åˆ° [0,1]
    if np.max(region_deformations) > 0:
        normalized_deformations = (region_deformations - np.min(region_deformations)) / (np.max(region_deformations) - np.min(region_deformations))
    else:
        normalized_deformations = region_deformations

    # æŒ‡æ•°æ”¾å¤§
    exp_deformations = np.exp(b * normalized_deformations)

    # å½’ä¸€åŒ–åˆ° [0, scale_max]
    scaled_exp_deformations = (exp_deformations / np.max(exp_deformations)) * scale_max if np.max(exp_deformations) > 0 else exp_deformations

    # è®¡ç®—å½¢å˜å æ¯”
    total_deformation = np.sum(scaled_exp_deformations)
    if total_deformation > 0:
        deformation_percentages = scaled_exp_deformations / total_deformation
    else:
        deformation_percentages = np.zeros_like(scaled_exp_deformations)

    # æ›´æ–°æ•°æ®å­—å…¸ï¼Œè¦†ç›–æ—§æ•°æ®
    data_dict[file_name] = deformation_percentages.tolist()

    # é‡æ–°å†™å…¥æ‰€æœ‰æ•°æ®ï¼ˆè¦†ç›–æ–‡ä»¶ï¼‰
    with open(outputfile_region, "w") as f:
        for key, values in data_dict.items():
            f.write(f"{key}," + ",".join(map(str, values)) + "\n")

    return deformation_percentages.tolist()


# åˆå§‹åŒ–ç‚¹äº‘çš„æ–¹æ³•ï¼ŒåŸºäºå›¾åƒçš„äºŒå€¼é¢ç§¯
def initialize_points_from_binary_area(area: torch.Tensor, n_points: int) -> torch.Tensor:
    sidelength = area.shape[0]
    points = []
    
    while len(points) < n_points:
        random_points = torch.FloatTensor(n_points, 2).uniform_(0, sidelength-1)
        idx = torch.round(random_points).long()
        valid_points = area[idx[:, 0], idx[:, 1]] == 1
        random_points = random_points[valid_points]
        
        if len(points) > 0:
            points = torch.cat([points, random_points], dim=0)
        else:
            points = random_points
            
    return points[:n_points]

# è¯»å–Açš„ç‚¹äº‘å›¾åƒ
def load_image_as_point_cloud(image_path, num_points):
    image = Image.open(image_path).convert("L")
    image = np.array(image)
    area = np.where(image > 128, 1, 0)
    area_tensor = torch.tensor(area, dtype=torch.float32)
    points = initialize_points_from_binary_area(area_tensor, num_points)
    return points.unsqueeze(0)  # è¿”å›å½¢çŠ¶ä¸º (1, num_points, 2) çš„ç‚¹äº‘

def compute_point_cloud_mapping_gpu_2(points_A, points_B, max_matches=2):
    # å°†è¾“å…¥å¼ é‡ç§»åŠ¨åˆ°CPUå¹¶è½¬æ¢ä¸ºNumPyæ•°ç»„
    points_A_np = points_A.cpu().numpy() if torch.is_tensor(points_A) else points_A
    points_B_np = points_B.cpu().numpy() if torch.is_tensor(points_B) else points_B
    
    n_A = points_A_np.shape[0]
    n_B = points_B_np.shape[0]
    
    # è®¡ç®—è·ç¦»çŸ©é˜µ
    dist_matrix = np.linalg.norm(points_A_np[:, np.newaxis] - points_B_np, axis=2)
    
    # åˆå§‹åŒ–åŒ¹é…è®¡æ•°å’Œç»“æœ
    count_B = np.zeros(n_B, dtype=int)
    matches = -np.ones(n_A, dtype=int)
    b_preference = np.zeros(n_B, dtype=float)
    
    # è´ªå¿ƒåŒ¹é…é€»è¾‘
    for i in range(n_A):
        sorted_b_indices = np.argsort(dist_matrix[i, :])
        total_distances = dist_matrix[i, sorted_b_indices] + b_preference[sorted_b_indices]
        
        for b_idx in sorted_b_indices[np.argsort(total_distances)]:
            if count_B[b_idx] < max_matches:
                matches[i] = b_idx
                count_B[b_idx] += 1
                b_preference[b_idx] += 30
                break
    
    return torch.tensor(matches)


# å¤„ç†æ¯å¼ å›¾ç‰‡å¹¶ç”Ÿæˆå½¢å˜åˆ†å¸ƒå›¾
def process_image_with_plot(file_name, template_points,region_labels, outputfile_region,output_all):
    image_path = os.path.join(particles_path, file_name)
    B_points = load_image_as_point_cloud(image_path, num_points=num_point).to(device)
    indices = compute_point_cloud_mapping_gpu_2(template_points.squeeze(0), B_points.squeeze(0))
    B_points = B_points[:, indices, :]
    
    # è®¡ç®—æ¯ä¸ªç‚¹çš„å½¢å˜å¤§å°
    point_deformations = torch.norm(B_points - template_points, dim=2).squeeze(0).cpu().numpy()
    total_deformation = point_deformations.mean()
    
    # è®¡ç®—æ¯ä¸ªåŒºåŸŸçš„å½¢å˜å æ¯”
    region_deformations = calculate_region_deformation(
        file_name,
        B_points.squeeze(0).detach().cpu().numpy(),
        template_points.squeeze(0).detach().cpu().numpy(),
        region_labels,
        outputfile_region
    )

    # æ›´æ–°æ¨¡æ¿å½¢å˜é‡
    updated_deformation = update_template_deformation(
        file_name,
        B_points,
        template_points,
        region_labels,
        deformation_accumulator,
        count_accumulator
    )

    # å¯è§†åŒ–æ¨¡æ¿å½¢å˜é‡
    plot_deformation_with_template(template_points, updated_deformation,output_all)

    return file_name, total_deformation


# ä¿®æ”¹ä¸»å‡½æ•°ï¼Œå¤„ç†æ¯å¼ å›¾ç‰‡å¹¶ä¿å­˜å›¾åƒ
def main():
    # åŠ è½½æ¨¡æ¿ç‚¹äº‘
    template_points = load_image_as_point_cloud(template_path, num_points=num_point).to(device)
    
    region_labels = divide_points_into_quadrants(template_points.squeeze(0).cpu().numpy(), img_width, img_height)

    # è·å–particles_pathç›®å½•ä¸‹æ‰€æœ‰pngæ–‡ä»¶
    image_files = [f for f in os.listdir(particles_path) if f.endswith('.png')]
    
    # å­˜å‚¨ç»“æœ
    results = []
    for file_name in tqdm(image_files, desc="Processing Images"):
        file_name, deformation = process_image_with_plot(file_name, template_points, region_labels, outputfile_region,output_all)
        results.append((file_name, deformation))
    
        # æŒ‰deformationå€¼é™åºæ’åº
        results.sort(key=lambda x: x[1], reverse=True)
        
        # ä¿å­˜åˆ°txtæ–‡ä»¶
        with open(output_file_path, 'w') as output_file:
            for file_name, deformation in results:
                output_file.write(f"{file_name} {deformation}\n")

if __name__ == "__main__":
    main()
